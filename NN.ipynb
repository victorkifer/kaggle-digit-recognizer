{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ################## Download and prepare the MNIST dataset ##################\n",
    "# This is just some way of getting the MNIST dataset from an online location\n",
    "# and loading it into numpy arrays. It doesn't involve Lasagne at all.\n",
    "\n",
    "def load_dataset1():\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_dataset2():\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "\n",
    "    with open('data/train.csv', 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            train_labels.append(int(row[0]))\n",
    "            train_features.append([int(x) for x in row[1:]])\n",
    "    \n",
    "    test_features = []\n",
    "\n",
    "    with open('data/test.csv', 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            test_features.append([int(x) for x in row])\n",
    "    \n",
    "    train_features = np.array(train_features)\n",
    "    train_features = train_features.reshape(-1, 1, 28, 28)\n",
    "    train_features = np.float32(train_features / np.float32(256))\n",
    "    \n",
    "    train_labels = np.int32(np.array(train_labels))\n",
    "    \n",
    "    test_features = np.array(test_features)\n",
    "    test_features = test_features.reshape(-1, 1, 28, 28)\n",
    "    test_features = np.float32(test_features / np.float32(256))\n",
    "    \n",
    "    return train_features, train_labels, None, None, test_features, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n",
      "ERROR:theano.sandbox.cuda:nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "#import copy\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ##################### Build the neural network model #######################\n",
    "# This script supports three types of models. For each one, we define a\n",
    "# function that takes a Theano variable representing the input and returns\n",
    "# the output layer of a neural network model built in Lasagne.\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "    # This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "    # a softmax output layer of 10 units. It applies 20% dropout to the input\n",
    "    # data and 50% dropout to the hidden layers.\n",
    "\n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    # (unspecified batchsize, 1 channel, 28 rows and 28 columns) and\n",
    "    # linking it to the given Theano variable `input_var`, if any:\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                     input_var=input_var)\n",
    "\n",
    "    # Apply 20% dropout to the input data:\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "\n",
    "    # Add a fully-connected layer of 800 units, using the linear rectifier, and\n",
    "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in_drop, num_units=800,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    # We'll now add dropout of 50%:\n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    # Another 800-unit layer:\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1_drop, num_units=800,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # 50% dropout again:\n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "\n",
    "    # Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid2_drop, num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    # Each layer is linked to its incoming layer(s), so we only need to pass\n",
    "    # the output layer to give access to a network in Lasagne:\n",
    "    return l_out\n",
    "\n",
    "\n",
    "def build_custom_mlp(input_var=None, depth=2, width=800, drop_input=.2,\n",
    "                     drop_hidden=.5):\n",
    "    # By default, this creates the same network as `build_mlp`, but it can be\n",
    "    # customized with respect to the number and size of hidden layers. This\n",
    "    # mostly showcases how creating a network in Python code can be a lot more\n",
    "    # flexible than a configuration file. Note that to make the code easier,\n",
    "    # all the layers are just called `network` -- there is no need to give them\n",
    "    # different names if all we return is the last one we created anyway; we\n",
    "    # just used different names above for clarity.\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "                network, width, nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 10, nonlinearity=softmax)\n",
    "    return network\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model='mlp'\n",
    "num_epochs=125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 125 took 10.829s\n",
      "Epoch 2 of 125 took 11.486s\n",
      "Epoch 3 of 125 took 14.266s\n",
      "Epoch 4 of 125 took 13.383s\n",
      "Epoch 5 of 125 took 14.701s\n",
      "Epoch 6 of 125 took 14.016s\n",
      "Epoch 7 of 125 took 15.168s\n",
      "Epoch 8 of 125 took 15.208s\n",
      "Epoch 9 of 125 took 15.176s\n",
      "Epoch 10 of 125 took 15.180s\n",
      "Epoch 11 of 125 took 14.220s\n",
      "Epoch 12 of 125 took 13.840s\n",
      "Epoch 13 of 125 took 13.152s\n",
      "Epoch 14 of 125 took 15.216s\n",
      "Epoch 15 of 125 took 15.192s\n",
      "Epoch 16 of 125 took 15.204s\n",
      "Epoch 17 of 125 took 14.956s\n",
      "Epoch 18 of 125 took 15.230s\n",
      "Epoch 19 of 125 took 15.250s\n",
      "Epoch 20 of 125 took 14.047s\n",
      "Epoch 21 of 125 took 14.844s\n",
      "Epoch 22 of 125 took 15.272s\n",
      "Epoch 23 of 125 took 15.132s\n",
      "Epoch 24 of 125 took 15.232s\n",
      "Epoch 25 of 125 took 15.280s\n",
      "Epoch 26 of 125 took 15.207s\n",
      "Epoch 27 of 125 took 15.143s\n",
      "Epoch 28 of 125 took 15.121s\n",
      "Epoch 29 of 125 took 15.457s\n",
      "Epoch 30 of 125 took 14.156s\n",
      "Epoch 31 of 125 took 15.027s\n",
      "Epoch 32 of 125 took 15.307s\n",
      "Epoch 33 of 125 took 15.459s\n",
      "Epoch 34 of 125 took 15.608s\n",
      "Epoch 35 of 125 took 15.444s\n",
      "Epoch 36 of 125 took 17.758s\n",
      "Epoch 37 of 125 took 19.730s\n",
      "Epoch 38 of 125 took 21.052s\n",
      "Epoch 39 of 125 took 15.660s\n",
      "Epoch 40 of 125 took 15.132s\n",
      "Epoch 41 of 125 took 14.640s\n",
      "Epoch 42 of 125 took 15.107s\n",
      "Epoch 43 of 125 took 16.245s\n",
      "Epoch 44 of 125 took 15.754s\n",
      "Epoch 45 of 125 took 14.697s\n",
      "Epoch 46 of 125 took 15.099s\n",
      "Epoch 47 of 125 took 14.928s\n",
      "Epoch 48 of 125 took 14.996s\n",
      "Epoch 49 of 125 took 15.054s\n",
      "Epoch 50 of 125 took 14.884s\n",
      "Epoch 51 of 125 took 15.265s\n",
      "Epoch 52 of 125 took 15.260s\n",
      "Epoch 53 of 125 took 15.292s\n",
      "Epoch 54 of 125 took 14.972s\n",
      "Epoch 55 of 125 took 14.273s\n",
      "Epoch 56 of 125 took 15.287s\n",
      "Epoch 57 of 125 took 15.060s\n",
      "Epoch 58 of 125 took 15.424s\n",
      "Epoch 59 of 125 took 15.115s\n",
      "Epoch 60 of 125 took 14.416s\n",
      "Epoch 61 of 125 took 15.620s\n",
      "Epoch 62 of 125 took 15.267s\n",
      "Epoch 63 of 125 took 15.311s\n",
      "Epoch 64 of 125 took 14.917s\n",
      "Epoch 65 of 125 took 15.025s\n",
      "Epoch 66 of 125 took 15.441s\n",
      "Epoch 67 of 125 took 15.485s\n",
      "Epoch 68 of 125 took 15.275s\n",
      "Epoch 69 of 125 took 15.307s\n",
      "Epoch 70 of 125 took 15.228s\n",
      "Epoch 71 of 125 took 15.250s\n",
      "Epoch 72 of 125 took 15.157s\n",
      "Epoch 73 of 125 took 15.295s\n",
      "Epoch 74 of 125 took 15.304s\n",
      "Epoch 75 of 125 took 15.383s\n",
      "Epoch 76 of 125 took 15.373s\n",
      "Epoch 77 of 125 took 15.197s\n",
      "Epoch 78 of 125 took 15.136s\n",
      "Epoch 79 of 125 took 15.315s\n",
      "Epoch 80 of 125 took 15.352s\n",
      "Epoch 81 of 125 took 15.314s\n",
      "Epoch 82 of 125 took 15.327s\n",
      "Epoch 83 of 125 took 15.516s\n",
      "Epoch 84 of 125 took 15.307s\n",
      "Epoch 85 of 125 took 15.381s\n",
      "Epoch 86 of 125 took 15.140s\n",
      "Epoch 87 of 125 took 15.315s\n",
      "Epoch 88 of 125 took 15.323s\n",
      "Epoch 89 of 125 took 15.168s\n",
      "Epoch 90 of 125 took 15.051s\n",
      "Epoch 91 of 125 took 15.008s\n",
      "Epoch 92 of 125 took 15.129s\n",
      "Epoch 93 of 125 took 15.256s\n",
      "Epoch 94 of 125 took 14.775s\n",
      "Epoch 95 of 125 took 15.297s\n",
      "Epoch 96 of 125 took 15.284s\n",
      "Epoch 97 of 125 took 14.334s\n",
      "Epoch 98 of 125 took 15.176s\n",
      "Epoch 99 of 125 took 15.266s\n",
      "Epoch 100 of 125 took 15.330s\n",
      "Epoch 101 of 125 took 15.538s\n",
      "Epoch 102 of 125 took 16.443s\n",
      "Epoch 103 of 125 took 15.221s\n",
      "Epoch 104 of 125 took 15.256s\n",
      "Epoch 105 of 125 took 15.311s\n",
      "Epoch 106 of 125 took 14.446s\n",
      "Epoch 107 of 125 took 14.539s\n",
      "Epoch 108 of 125 took 15.249s\n",
      "Epoch 109 of 125 took 15.376s\n",
      "Epoch 110 of 125 took 14.575s\n",
      "Epoch 111 of 125 took 15.230s\n",
      "Epoch 112 of 125 took 15.344s\n",
      "Epoch 113 of 125 took 15.366s\n",
      "Epoch 114 of 125 took 15.367s\n",
      "Epoch 115 of 125 took 15.309s\n",
      "Epoch 116 of 125 took 15.141s\n",
      "Epoch 117 of 125 took 15.365s\n",
      "Epoch 118 of 125 took 15.402s\n",
      "Epoch 119 of 125 took 15.412s\n",
      "Epoch 120 of 125 took 15.349s\n",
      "Epoch 121 of 125 took 15.353s\n",
      "Epoch 122 of 125 took 15.880s\n",
      "Epoch 123 of 125 took 15.253s\n",
      "Epoch 124 of 125 took 14.599s\n",
      "Epoch 125 of 125 took 15.400s\n"
     ]
    }
   ],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "    \n",
    "print(\"Building model and compiling functions...\")\n",
    "if model == 'mlp':\n",
    "    network = build_mlp(input_var)\n",
    "elif model.startswith('custom_mlp:'):\n",
    "    depth, width, drop_in, drop_hid = model.split(':', 1)[1].split(',')\n",
    "    network = build_custom_mlp(input_var, int(depth), int(width),\n",
    "                                   float(drop_in), float(drop_hid))\n",
    "elif model == 'cnn':\n",
    "    network = build_cnn(input_var)\n",
    "else:\n",
    "    print(\"Unrecognized model type %r.\" % model)\n",
    "    exit(-1)\n",
    "    \n",
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "# We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "# predict_fn = theano.function([input_var], test_prediction)\n",
    "predict_fn = theano.function([input_var], T.argmax(test_prediction, axis=1))\n",
    "\n",
    "  \n",
    "loss_train = []\n",
    "loss_val =  []\n",
    "accuracy_val = []\n",
    "predicted_value = []\n",
    "formatted_decisions = []\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "#     val_err = 0\n",
    "#     val_acc = 0\n",
    "#     val_batches = 0\n",
    "#     for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "#         inputs, targets = batch\n",
    "#         err, acc = val_fn(inputs, targets)\n",
    "#         val_err += err\n",
    "#         val_acc += acc\n",
    "#         val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "\n",
    "#     train_loss = train_err / train_batches        \n",
    "#     valid_loss = val_err / val_batches\n",
    "#     valid_accuracy = val_acc / val_batches * 100\n",
    "        \n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "#     print(\"  training loss:\\t\\t{:.6f}\".format(train_loss))\n",
    "#     print(\"  validation loss:\\t\\t{:.6f}\".format(valid_loss))\n",
    "#     print(\"  validation accuracy:\\t\\t{:.2f} %\".format(valid_accuracy))\n",
    "                \n",
    "        \n",
    "#     loss_train.append(train_loss)      \n",
    "#     loss_val.append(valid_loss)        \n",
    "#     accuracy_val.append(valid_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network.save_weights_to('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "\n",
    "for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "    predicted_value.append(predict_fn(inputs))\n",
    "    count = 1\n",
    "    for decision in predicted_value:\n",
    "        formatted_decisions.append(decision)\n",
    "        count += 1  \n",
    "    \n",
    "testing_loss = test_err / test_batches\n",
    "testing_accuracy = test_acc / test_batches * 100    \n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(testing_loss))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format( testing_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_features = []\n",
    "\n",
    "# with open('data/test.csv', 'rb') as csvfile:\n",
    "#     reader = csv.reader(csvfile, delimiter=',')\n",
    "#     next(reader, None)\n",
    "#     for row in reader:\n",
    "#         test_features.append([int(x) for x in row])\n",
    "\n",
    "# test_features = np.array(test_features)\n",
    "# test_features = test_features.reshape(-1, 1, 28, 28)\n",
    "# test_features = np.float32(test_features / np.float32(256))\n",
    "\n",
    "# values = predict_fn(test_features)\n",
    "values = predict_fn(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('result.csv', 'wb+') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(['ImageId', 'Label'])\n",
    "        index = 1\n",
    "        for item in values:\n",
    "            # print(\"{} \\t {} \\n\" .format(index, item))\n",
    "            writer.writerow([index, item])\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     plt.figure(figsize=(25,10))\n",
    "#     plt.figure(1)\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title('Train and Validation Loss')\n",
    "#     plt.plot(loss_train, color='blue', label='train loss')\n",
    "#     plt.plot(loss_val, color='red', label='validation loss')\n",
    "#     plt.legend(loc='upper right')t\n",
    "#     plt.figure(figsize=(25,10))\n",
    "#     plt.figure(2)\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.title('Validation Accuracy')\n",
    "#     plt.plot(accuracy_val, color='green')\n",
    "    \n",
    "#     plt.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
